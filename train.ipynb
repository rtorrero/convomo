{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Emotion Recognition using Transfer Learning with Inception V3\n",
    "\n",
    "This notebook demonstrates how to build, train and evaluate a Convolutional Neural Network (CNN) for facial emotion recognition by leveraging transfer learning with the pre-trained Inception V3 architecture.\n",
    "\n",
    "The model will learn to classify facial expressions into different emotion categories like happy, sad, angry, neutral, etc. using the power of deep learning and computer vision.\n",
    "\n",
    "### Setup Instructions\n",
    "\n",
    "#### For NVIDIA GPU / CPU Users\n",
    "By default the next cell will install the required dependencies for CPU users or CUDA users. If you are running this notebook on ROCm, please comment the first line and uncomment the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "#!pip install -r requirements.rocm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Use WanDB to monitor your training process\n",
    "If you have a wandb account, you can use it to monitor your training process. You can install wandb using pip:  \n",
    "```\n",
    "pip install wandb\n",
    "```\n",
    "and configure your wandb account using the following command:\n",
    "```\n",
    "wandb login\n",
    "```\n",
    "After this, the notebook will automatically log your training process to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "def init_wandb(project_name, config):\n",
    "    if WANDB_AVAILABLE:\n",
    "        return wandb.init(project=project_name, config=config);\n",
    "    return None\n",
    "\n",
    "def log_metrics(metrics, step=None):\n",
    "    if WANDB_AVAILABLE:\n",
    "        wandb.log(metrics, step=step);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for Emotion Recognition\n",
    "\n",
    "This function sets up data loading and preprocessing for an emotion recognition model:\n",
    "\n",
    "- Uses PyTorch's ImageFolder to load emotion images\n",
    "- Applies two types of transforms to increase data diversity:\n",
    "  - Training: Includes augmentation (flips, rotations, color adjustments)\n",
    "  - Testing/Validation: Basic resizing and normalization only\n",
    "- Splits dataset into:\n",
    "  - 70% training\n",
    "  - 15% validation\n",
    "  - 15% testing\n",
    "- Creates DataLoaders with batch size 128\n",
    "- All images are processed to 299x299 (Inception V3 size)\n",
    "- Uses ImageNet normalization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "def setup_data_loaders(batch_size=64):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Validation/Testing Transformations (no augmentation but with resizing)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and std\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.ImageFolder(root='emotions_dataset/emotions_dataset_cropped_faces', transform=None)\n",
    "\n",
    "    # Split the dataset into training, validation, and testing sets\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    val_size = int(0.15 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    # Apply the previously created transforms\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = test_transform\n",
    "    test_dataset.dataset.transform = test_transform\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Print dataset sizes for confirmation\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, full_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "This code block will define the training and validation phases of the model. It:\n",
    "\n",
    "- Processes batches of images through the model\n",
    "- Calculates losses and performs backpropagation\n",
    "- Updates model weights using the optimizer\n",
    "\n",
    "The validation phase evaluates the model's performance by:\n",
    "\n",
    "- Computing validation loss\n",
    "- Calculating prediction accuracy\n",
    "- Tracking metrics like train loss, validation loss, and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        log_metrics({\"train_loss\": train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Inception V3\n",
    "This is a pre-trained model that has been trained on the ImageNet dataset. This function will load the model and return it. We disable the aux_logits parameter as it is not needed for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Inception V3\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "# Load and modify the model\n",
    "def get_inception_model(num_classes):\n",
    "    model = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "    model.aux_logits = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model\n",
    "A few parameters can be configured to change the behavior of the model:\n",
    "- num_epochs: number of epochs to train the model\n",
    "- batch_size: number of samples per batch\n",
    "- learning_rate: learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 857\n",
      "Validation samples: 183\n",
      "Testing samples: 185\n",
      "Using device: cuda for model training\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.000025\n",
    "\n",
    "# Use the loaders defined above to prepare the data for training\n",
    "train_loader, val_loader, test_loader, classes = setup_data_loaders(batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} for model training\")\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = get_inception_model(num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "We are now ready to start training our model. If wandb is installed, it will automatically log the training metrics to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 1.7352, Val Loss: 1.7058, Val Accuracy: 27.87%\n",
      "Epoch [2/30], Train Loss: 1.5247, Val Loss: 1.6002, Val Accuracy: 38.25%\n",
      "Epoch [3/30], Train Loss: 1.3263, Val Loss: 1.5105, Val Accuracy: 39.34%\n",
      "Epoch [4/30], Train Loss: 1.1541, Val Loss: 1.4084, Val Accuracy: 48.63%\n",
      "Epoch [5/30], Train Loss: 0.9924, Val Loss: 1.3165, Val Accuracy: 52.46%\n",
      "Epoch [6/30], Train Loss: 0.8285, Val Loss: 1.2370, Val Accuracy: 54.64%\n",
      "Epoch [7/30], Train Loss: 0.6683, Val Loss: 1.1768, Val Accuracy: 53.55%\n",
      "Epoch [8/30], Train Loss: 0.5473, Val Loss: 1.1316, Val Accuracy: 55.74%\n",
      "Epoch [9/30], Train Loss: 0.4324, Val Loss: 1.0952, Val Accuracy: 56.28%\n",
      "Epoch [10/30], Train Loss: 0.3433, Val Loss: 1.0758, Val Accuracy: 59.02%\n",
      "Epoch [11/30], Train Loss: 0.2778, Val Loss: 1.0552, Val Accuracy: 61.20%\n",
      "Epoch [12/30], Train Loss: 0.2159, Val Loss: 1.0515, Val Accuracy: 59.02%\n",
      "Epoch [13/30], Train Loss: 0.1640, Val Loss: 1.0327, Val Accuracy: 61.75%\n",
      "Epoch [14/30], Train Loss: 0.1387, Val Loss: 1.0317, Val Accuracy: 60.11%\n",
      "Epoch [15/30], Train Loss: 0.1159, Val Loss: 1.0342, Val Accuracy: 60.66%\n",
      "Epoch [16/30], Train Loss: 0.0845, Val Loss: 1.0169, Val Accuracy: 61.75%\n",
      "Epoch [17/30], Train Loss: 0.0724, Val Loss: 1.0155, Val Accuracy: 63.39%\n",
      "Epoch [18/30], Train Loss: 0.0686, Val Loss: 1.0113, Val Accuracy: 61.20%\n",
      "Epoch [19/30], Train Loss: 0.0542, Val Loss: 0.9944, Val Accuracy: 63.39%\n",
      "Epoch [20/30], Train Loss: 0.0461, Val Loss: 0.9968, Val Accuracy: 63.39%\n",
      "Epoch [21/30], Train Loss: 0.0570, Val Loss: 1.0190, Val Accuracy: 61.20%\n",
      "Epoch [22/30], Train Loss: 0.0383, Val Loss: 1.0080, Val Accuracy: 63.93%\n",
      "Epoch [23/30], Train Loss: 0.0376, Val Loss: 1.0062, Val Accuracy: 64.48%\n",
      "Epoch [24/30], Train Loss: 0.0304, Val Loss: 1.0086, Val Accuracy: 64.48%\n",
      "Epoch [25/30], Train Loss: 0.0304, Val Loss: 1.0060, Val Accuracy: 64.48%\n",
      "Epoch [26/30], Train Loss: 0.0261, Val Loss: 1.0189, Val Accuracy: 65.03%\n",
      "Epoch [27/30], Train Loss: 0.0298, Val Loss: 1.0132, Val Accuracy: 64.48%\n",
      "Epoch [28/30], Train Loss: 0.0239, Val Loss: 1.0049, Val Accuracy: 65.57%\n",
      "Epoch [29/30], Train Loss: 0.0270, Val Loss: 1.0257, Val Accuracy: 63.39%\n",
      "Epoch [30/30], Train Loss: 0.0223, Val Loss: 1.0219, Val Accuracy: 63.93%\n"
     ]
    }
   ],
   "source": [
    "# Attempt to init wandb\n",
    "init_wandb(\"convomo\", config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"yousefmohamed20/sentiment-images-classifier\",\n",
    "    \"epochs\": num_epochs,\n",
    "})\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.32%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>██▇▆▅▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▃▃▅▆▆▆▆▆▇▇▇▇▇▇▇█▇██▇█████████</td></tr><tr><td>val_loss</td><td>██▇▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.02234</td></tr><tr><td>val_accuracy</td><td>63.93443</td></tr><tr><td>val_loss</td><td>1.02186</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-plasma-11</strong> at: <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/10zvifhn' target=\"_blank\">https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/10zvifhn</a><br> View project at: <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo' target=\"_blank\">https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241221_204223-10zvifhn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model\n",
    "evaluate_model(trained_model, test_loader, device)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
