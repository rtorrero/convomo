{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Emotion Recognition using Transfer Learning with Inception V3\n",
    "\n",
    "This notebook demonstrates how to build, train and evaluate a Convolutional Neural Network (CNN) for facial emotion recognition by leveraging transfer learning with the pre-trained Inception V3 architecture.\n",
    "\n",
    "The model will learn to classify facial expressions into different emotion categories like happy, sad, angry, neutral, etc. using the power of deep learning and computer vision.\n",
    "\n",
    "### Setup Instructions\n",
    "\n",
    "#### For NVIDIA GPU / CPU Users\n",
    "By default the next cell will install the required dependencies for CPU users or CUDA users. If you are running this notebook on ROCm, please comment the first line and uncomment the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types==0.7.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: certifi==2024.12.14 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: click==8.1.7 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: comm==0.2.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: debugpy==1.8.11 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.8.11)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: docker-pycreds==0.4.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: executing==2.1.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: filelock==3.16.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (3.16.1)\n",
      "Requirement already satisfied: fsspec==2024.12.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2024.12.0)\n",
      "Requirement already satisfied: gitdb==4.0.11 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (4.0.11)\n",
      "Requirement already satisfied: GitPython==3.1.43 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (3.1.43)\n",
      "Requirement already satisfied: idna==3.10 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.31.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (8.31.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (3.1.4)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (5.7.2)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (3.4.2)\n",
      "Requirement already satisfied: numpy==2.2.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 32)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 33)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 35)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 37)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 38)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 39)) (12.4.127)\n",
      "Requirement already satisfied: packaging==24.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (24.2)\n",
      "Requirement already satisfied: parso==0.8.4 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 41)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 42)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.0.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 43)) (11.0.0)\n",
      "Requirement already satisfied: pip==24.3.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 44)) (24.3.1)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 45)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 46)) (3.0.48)\n",
      "Requirement already satisfied: protobuf==5.29.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 47)) (5.29.2)\n",
      "Requirement already satisfied: psutil==6.1.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 48)) (6.1.1)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 49)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 50)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.10.4 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 51)) (2.10.4)\n",
      "Requirement already satisfied: pydantic_core==2.27.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 52)) (2.27.2)\n",
      "Requirement already satisfied: Pygments==2.18.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 53)) (2.18.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 54)) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 55)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 56)) (26.2.0)\n",
      "Requirement already satisfied: requests==2.32.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 57)) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk==2.19.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 58)) (2.19.2)\n",
      "Requirement already satisfied: setproctitle==1.3.4 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 59)) (1.3.4)\n",
      "Requirement already satisfied: setuptools==75.6.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 60)) (75.6.0)\n",
      "Requirement already satisfied: six==1.17.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 61)) (1.17.0)\n",
      "Requirement already satisfied: smmap==5.0.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 62)) (5.0.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 63)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 64)) (1.13.1)\n",
      "Requirement already satisfied: torch==2.5.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 65)) (2.5.1+rocm6.2)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 66)) (2.5.1+rocm6.2)\n",
      "Requirement already satisfied: torchvision==0.20.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 67)) (0.20.1+rocm6.2)\n",
      "Requirement already satisfied: tornado==6.4.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 68)) (6.4.2)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 69)) (5.14.3)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 70)) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 71)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.2.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 72)) (2.2.3)\n",
      "Requirement already satisfied: wandb==0.19.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 73)) (0.19.1)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 74)) (0.2.13)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 65)) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "#!pip install -r requirements.rocm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Use WanDB to monitor your training process\n",
    "If you have a wandb account, you can use it to monitor your training process. You can install wandb using pip:  \n",
    "```\n",
    "pip install wandb\n",
    "```\n",
    "and configure your wandb account using the following command:\n",
    "```\n",
    "wandb login\n",
    "```\n",
    "After this, the notebook will automatically log your training process to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "def init_wandb(project_name, config):\n",
    "    if WANDB_AVAILABLE:\n",
    "        return wandb.init(project=project_name, config=config)\n",
    "    return None\n",
    "\n",
    "def log_metrics(metrics, step=None):\n",
    "    if WANDB_AVAILABLE:\n",
    "        wandb.log(metrics, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for Emotion Recognition\n",
    "\n",
    "This function sets up data loading and preprocessing for an emotion recognition model:\n",
    "\n",
    "- Uses PyTorch's ImageFolder to load emotion images\n",
    "- Applies two types of transforms to increase data diversity:\n",
    "  - Training: Includes augmentation (flips, rotations, color adjustments)\n",
    "  - Testing/Validation: Basic resizing and normalization only\n",
    "- Splits dataset into:\n",
    "  - 70% training\n",
    "  - 15% validation\n",
    "  - 15% testing\n",
    "- Creates DataLoaders with batch size 128\n",
    "- All images are processed to 299x299 (Inception V3 size)\n",
    "- Uses ImageNet normalization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "def setup_data_loaders(batch_size=64):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Validation/Testing Transformations (no augmentation but with resizing)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and std\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.ImageFolder(root='emotions_dataset/emotions_dataset_cropped_faces', transform=None)\n",
    "\n",
    "    # Split the dataset into training, validation, and testing sets\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    val_size = int(0.15 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    # Apply the previously created transforms\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = test_transform\n",
    "    test_dataset.dataset.transform = test_transform\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Print dataset sizes for confirmation\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, full_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "This code block will define the training and validation phases of the model. It:\n",
    "\n",
    "- Processes batches of images through the model\n",
    "- Calculates losses and performs backpropagation\n",
    "- Updates model weights using the optimizer\n",
    "\n",
    "The validation phase evaluates the model's performance by:\n",
    "\n",
    "- Computing validation loss\n",
    "- Calculating prediction accuracy\n",
    "- Tracking metrics like train loss, validation loss, and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        log_metrics({\"train_loss\": train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Inception V3\n",
    "This is a pre-trained model that has been trained on the ImageNet dataset. This function will load the model and return it. We disable the aux_logits parameter as it is not needed for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Inception V3\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "# Load and modify the model\n",
    "def get_inception_model(num_classes):\n",
    "    model = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "    model.aux_logits = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model\n",
    "A few parameters can be configured to change the behavior of the model:\n",
    "- num_epochs: number of epochs to train the model\n",
    "- batch_size: number of samples per batch\n",
    "- learning_rate: learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 857\n",
      "Validation samples: 183\n",
      "Testing samples: 185\n",
      "Using device: cuda for model training\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.000025\n",
    "\n",
    "# Use the loaders defined above to prepare the data for training\n",
    "train_loader, val_loader, test_loader, classes = setup_data_loaders(batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} for model training\")\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = get_inception_model(num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "We are now ready to start training our model. If wandb is installed, it will automatically log the training metrics to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrtorreromarijnissen\u001b[0m (\u001b[33mrtorreromarijnissen-university-of-las-palmas-de-gran-canaria\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rtorrero/Workspace/convomo/wandb/run-20241221_200129-yidtsty8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/yidtsty8' target=\"_blank\">stoic-wildflower-10</a></strong> to <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo' target=\"_blank\">https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/yidtsty8' target=\"_blank\">https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/yidtsty8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtorrero/Workspace/convomo/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at ../aten/src/ATen/Context.cpp:296.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 1.7964, Val Loss: 1.7117, Val Accuracy: 26.23%\n",
      "Epoch [2/30], Train Loss: 1.5956, Val Loss: 1.6047, Val Accuracy: 44.81%\n",
      "Epoch [3/30], Train Loss: 1.4034, Val Loss: 1.4972, Val Accuracy: 49.73%\n",
      "Epoch [4/30], Train Loss: 1.2356, Val Loss: 1.3982, Val Accuracy: 54.10%\n",
      "Epoch [5/30], Train Loss: 1.0493, Val Loss: 1.2920, Val Accuracy: 57.92%\n",
      "Epoch [6/30], Train Loss: 0.8976, Val Loss: 1.2066, Val Accuracy: 61.75%\n",
      "Epoch [7/30], Train Loss: 0.7183, Val Loss: 1.1310, Val Accuracy: 66.12%\n",
      "Epoch [8/30], Train Loss: 0.6106, Val Loss: 1.0605, Val Accuracy: 67.21%\n",
      "Epoch [9/30], Train Loss: 0.4659, Val Loss: 1.0156, Val Accuracy: 68.31%\n",
      "Epoch [10/30], Train Loss: 0.3926, Val Loss: 0.9813, Val Accuracy: 67.21%\n",
      "Epoch [11/30], Train Loss: 0.2985, Val Loss: 0.9535, Val Accuracy: 68.85%\n",
      "Epoch [12/30], Train Loss: 0.2365, Val Loss: 0.9366, Val Accuracy: 68.85%\n",
      "Epoch [13/30], Train Loss: 0.1905, Val Loss: 0.9276, Val Accuracy: 70.49%\n",
      "Epoch [14/30], Train Loss: 0.1533, Val Loss: 0.9123, Val Accuracy: 68.85%\n",
      "Epoch [15/30], Train Loss: 0.1311, Val Loss: 0.9197, Val Accuracy: 69.95%\n",
      "Epoch [16/30], Train Loss: 0.0991, Val Loss: 0.9180, Val Accuracy: 68.31%\n",
      "Epoch [17/30], Train Loss: 0.0785, Val Loss: 0.9098, Val Accuracy: 69.40%\n",
      "Epoch [18/30], Train Loss: 0.0676, Val Loss: 0.9024, Val Accuracy: 69.40%\n",
      "Epoch [19/30], Train Loss: 0.0608, Val Loss: 0.9041, Val Accuracy: 68.85%\n",
      "Epoch [20/30], Train Loss: 0.0555, Val Loss: 0.9126, Val Accuracy: 68.85%\n",
      "Epoch [21/30], Train Loss: 0.0467, Val Loss: 0.9083, Val Accuracy: 68.31%\n",
      "Epoch [22/30], Train Loss: 0.0413, Val Loss: 0.9160, Val Accuracy: 69.40%\n",
      "Epoch [23/30], Train Loss: 0.0408, Val Loss: 0.9113, Val Accuracy: 69.40%\n",
      "Epoch [24/30], Train Loss: 0.0295, Val Loss: 0.9146, Val Accuracy: 71.04%\n",
      "Epoch [25/30], Train Loss: 0.0296, Val Loss: 0.9124, Val Accuracy: 71.04%\n",
      "Epoch [26/30], Train Loss: 0.0294, Val Loss: 0.9079, Val Accuracy: 71.04%\n",
      "Epoch [27/30], Train Loss: 0.0279, Val Loss: 0.8908, Val Accuracy: 71.58%\n",
      "Epoch [28/30], Train Loss: 0.0259, Val Loss: 0.9099, Val Accuracy: 70.49%\n",
      "Epoch [29/30], Train Loss: 0.0241, Val Loss: 0.9294, Val Accuracy: 69.95%\n",
      "Epoch [30/30], Train Loss: 0.0216, Val Loss: 0.9353, Val Accuracy: 69.40%\n"
     ]
    }
   ],
   "source": [
    "# Attempt to init wandb\n",
    "init_wandb(\"convomo\", config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"yousefmohamed20/sentiment-images-classifier\",\n",
    "    \"epochs\": num_epochs,\n",
    "})\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.03%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▇▆▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇▇▇█████▇████▇█████████</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.02159</td></tr><tr><td>val_accuracy</td><td>69.39891</td></tr><tr><td>val_loss</td><td>0.93529</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-wildflower-10</strong> at: <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/yidtsty8' target=\"_blank\">https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo/runs/yidtsty8</a><br> View project at: <a href='https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo' target=\"_blank\">https://wandb.ai/rtorreromarijnissen-university-of-las-palmas-de-gran-canaria/convomo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241221_200129-yidtsty8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model\n",
    "evaluate_model(trained_model, test_loader, device)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
